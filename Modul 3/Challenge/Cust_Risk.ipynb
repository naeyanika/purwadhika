{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MACHINE LEARNING CHALLENGE: ANALISIS RISIKO CUSTOMER E-COMMERCE INDONESIA\n",
    "\n",
    "## 1. Latar Belakang\n",
    "Sebuah perusahaan e-commerce lokal Indonesia sedang mengalami peningkatan jumlah customer baru. Namun, tim operasional menemukan masalah:\n",
    "- Ada customer yang sering membatalkan order, mengakibatkan kerugian ongkir.\n",
    "- Ada customer yang pembayaran COD-nya sering gagal, sehingga barang harus dikembalikan.\n",
    "- Beberapa customer melakukan return berulang kali dengan alasan yang tidak jelas.\n",
    "\n",
    "## 2. Goals\n",
    "Tujuan utama adalah membangun model machine learning untuk memprediksi perilaku risiko customer berdasarkan histori transaksi dan interaksi mereka. Model ini akan digunakan untuk:\n",
    "- Menentukan kelayakan COD.\n",
    "- Menentukan order yang perlu verifikasi manual.\n",
    "- Menyusun strategi retensi.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Preprocessing & Model Selection\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Imbalance Handling\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import (f1_score, recall_score, precision_score, roc_auc_score, \n",
    "                             confusion_matrix, classification_report, accuracy_score, make_scorer)\n",
    "\n",
    "# Display Options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Understanding\n",
    "Insight awal dari dataset: distribusi variabel, missing value, outlier, dan korelasi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Shape: (10214, 13)\n",
      "\n",
      "Info Dataset:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10214 entries, 0 to 10213\n",
      "Data columns (total 13 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   customer_id        10214 non-null  object \n",
      " 1   age                7627 non-null   object \n",
      " 2   registration_date  10214 non-null  object \n",
      " 3   city               10214 non-null  object \n",
      " 4   total_orders       5134 non-null   float64\n",
      " 5   cancel_rate        7619 non-null   object \n",
      " 6   return_count       10214 non-null  object \n",
      " 7   cod_failed         8567 non-null   object \n",
      " 8   avg_order_value    10214 non-null  float64\n",
      " 9   last_purchase      10214 non-null  object \n",
      " 10  device             10214 non-null  object \n",
      " 11  complaints         8123 non-null   object \n",
      " 12  risk_flag          4110 non-null   float64\n",
      "dtypes: float64(3), object(10)\n",
      "memory usage: 1.0+ MB\n",
      "\n",
      "Missing Values:\n",
      "customer_id             0\n",
      "age                  2587\n",
      "registration_date       0\n",
      "city                    0\n",
      "total_orders         5080\n",
      "cancel_rate          2595\n",
      "return_count            0\n",
      "cod_failed           1647\n",
      "avg_order_value         0\n",
      "last_purchase           0\n",
      "device                  0\n",
      "complaints           2091\n",
      "risk_flag            6104\n",
      "dtype: int64\n",
      "\n",
      "Statistik Deskriptif:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_orders</th>\n",
       "      <th>avg_order_value</th>\n",
       "      <th>risk_flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5134.000000</td>\n",
       "      <td>1.021400e+04</td>\n",
       "      <td>4110.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>23.321582</td>\n",
       "      <td>5.130431e+07</td>\n",
       "      <td>0.503163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>15.267024</td>\n",
       "      <td>4.876601e+07</td>\n",
       "      <td>0.500051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-3.000000</td>\n",
       "      <td>1.937060e+03</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>2.494366e+06</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>23.000000</td>\n",
       "      <td>1.000000e+08</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>37.000000</td>\n",
       "      <td>1.000000e+08</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>49.000000</td>\n",
       "      <td>1.000000e+08</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       total_orders  avg_order_value    risk_flag\n",
       "count   5134.000000     1.021400e+04  4110.000000\n",
       "mean      23.321582     5.130431e+07     0.503163\n",
       "std       15.267024     4.876601e+07     0.500051\n",
       "min       -3.000000     1.937060e+03     0.000000\n",
       "25%       10.000000     2.494366e+06     0.000000\n",
       "50%       23.000000     1.000000e+08     1.000000\n",
       "75%       37.000000     1.000000e+08     1.000000\n",
       "max       49.000000     1.000000e+08     1.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample cancel_rate values (raw):\n",
      "cancel_rate\n",
      "0,2                   2584\n",
      "20%                   2535\n",
      "8.745310152178726        1\n",
      "125.7689757796603        1\n",
      "63.75177707438059        1\n",
      "82.68268528447099        1\n",
      "1.0350375942248358       1\n",
      "73.39262538295903        1\n",
      "8.129820480415173        1\n",
      "60.33476790016527        1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('Customer Risk Dataset.csv')\n",
    "\n",
    "print(\"Dataset Shape:\", df.shape)\n",
    "print(\"\\nInfo Dataset:\")\n",
    "df.info()\n",
    "print(\"\\nMissing Values:\")\n",
    "print(df.isnull().sum())\n",
    "print(\"\\nStatistik Deskriptif:\")\n",
    "display(df.describe())\n",
    "\n",
    "print(\"\\nSample cancel_rate values (raw):\")\n",
    "print(df['cancel_rate'].value_counts().head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Menentukan Analisis (Regresi/Klasifikasi)\n",
    "\n",
    "**Problem Statement Final:**\n",
    "Memprediksi apakah seorang customer berpotensi \"Berisiko\" (Gagal COD/Sering Cancel/Return) atau \"Tidak Berisiko\".\n",
    "\n",
    "**Jenis Analisis: Klasifikasi**\n",
    "**Alasan:**\n",
    "1. Output yang diinginkan adalah label kategori biner: Berisiko (1) atau Tidak Berisiko (0).\n",
    "2. Bisnis perlu keputusan tegas (Ya/Tidak) untuk menyetujui fitur COD atau melakukan verifikasi manual.\n",
    "3. Meskipun kita bisa memprediksi probabilitas (skor risiko), pada akhirnya operasional butuh threshold untuk mengambil tindakan.\n",
    "\n",
    "**Penanganan Label:**\n",
    "Kolom `risk_flag` sudah tersedia sebagai label. Jika label ini kotor (missing), kita akan melakukan imputasi (mengisi dengan 0/Safe) atau membuang baris tersebut. Dalam kasus ini, kita asumsikan missing label berarti belum ada insiden risiko, jadi kita isi dengan 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Menentukan Metrics + Alasan Memilihnya\n",
    "\n",
    "**Metric Utama: Recall (Sensitivitas)**\n",
    "\n",
    "**Alasan:**\n",
    "- **Kasus Bisnis**: Kita ingin mendeteksi customer yang berpotensi merugikan (COD gagal bayar).\n",
    "- **False Negative (FN)**: Model memprediksi Aman (0), padahal aslinya Berisiko (1). \n",
    "  - *Dampak*: Perusahaan mengirim barang COD -> Customer tidak bayar -> Perusahaan rugi Ongkir + Biaya Retur + Potensi Barang Rusak.\n",
    "- **False Positive (FP)**: Model memprediksi Berisiko (1), padahal aslinya Aman (0).\n",
    "  - *Dampak*: Customer baik ditolak COD-nya -> Potensi kehilangan penjualan (Opportunity Cost).\n",
    "\n",
    "Dalam konteks manajemen risiko, **meminimalkan kerugian nyata (FN)** biasanya lebih prioritas daripada kehilangan potensi untung (FP). Oleh karena itu, kita ingin **Recall setinggi mungkin** untuk kelas positif (Risk).\n",
    "\n",
    "Namun, kita juga akan memantau **F1-Score** dan **ROC-AUC** untuk memastikan model tidak sekadar memprediksi semua orang sebagai risiko (yang akan membuat Recall 100% tapi Precision hancur)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Cleaning\n",
    "Meliputi perbaikan typo, standardisasi kategori, penanganan missing value, dan perbaikan format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cancel rate range: 0.00 - 99.99\n",
      "count    10214.000000\n",
      "mean        19.500922\n",
      "std         20.004446\n",
      "min          0.000000\n",
      "25%          0.000000\n",
      "50%         20.000000\n",
      "75%         20.000000\n",
      "max         99.990000\n",
      "Name: cancel_rate, dtype: float64\n",
      "\n",
      "Risk flag distribution after imputation:\n",
      "risk_flag\n",
      "0    6048\n",
      "1    4166\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Data Cleaning Selesai.\n"
     ]
    }
   ],
   "source": [
    "df_clean = df.copy()\n",
    "\n",
    "# --- 1. Age Cleaning ---\n",
    "num_words = {\n",
    "    \"nol\": 0, \"satu\": 1, \"dua\": 2, \"tiga\": 3, \"empat\": 4,\n",
    "    \"lima\": 5, \"enam\": 6, \"tujuh\": 7, \"delapan\": 8,\n",
    "    \"sembilan\": 9, \"sepuluh\": 10, \"sebelas\": 11\n",
    "}\n",
    "\n",
    "def text_to_number(text):\n",
    "    if pd.isna(text):\n",
    "        return np.nan\n",
    "    text = str(text).lower().strip()\n",
    "    digits = re.findall(r'\\d+', text)\n",
    "    if digits:\n",
    "        return int(digits[0])\n",
    "    parts = text.split()\n",
    "    if \"puluh\" in parts:\n",
    "        idx = parts.index(\"puluh\")\n",
    "        tens = num_words.get(parts[idx - 1], 0) * 10\n",
    "        if idx + 1 < len(parts):\n",
    "            return tens + num_words.get(parts[idx + 1], 0)\n",
    "        return tens\n",
    "    if \"belas\" in text:\n",
    "        return num_words.get(parts[0], 0) + 10\n",
    "    if text in num_words:\n",
    "        return num_words[text]\n",
    "    return np.nan\n",
    "\n",
    "df_clean[\"age\"] = df_clean[\"age\"].apply(text_to_number)\n",
    "df_clean[\"age\"] = df_clean[\"age\"].fillna(df_clean[\"age\"].median()).astype(int)\n",
    "\n",
    "# --- 2. Date Parsing ---\n",
    "def parse_date(date_str):\n",
    "    if pd.isna(date_str) or date_str == '':\n",
    "        return pd.NaT\n",
    "    formats = ['%B %d %Y', '%d-%m-%y', '%Y/%m/%d', '%m/%d/%Y', '%b %d %Y']\n",
    "    date_str = str(date_str).strip()\n",
    "    for fmt in formats:\n",
    "        try:\n",
    "            return pd.to_datetime(date_str, format=fmt)\n",
    "        except:\n",
    "            pass\n",
    "    try:\n",
    "        return pd.to_datetime(date_str)\n",
    "    except:\n",
    "        return pd.NaT\n",
    "\n",
    "df_clean[\"registration_date\"] = df_clean[\"registration_date\"].map(parse_date)\n",
    "df_clean[\"last_purchase\"] = df_clean[\"last_purchase\"].map(parse_date)\n",
    "\n",
    "# --- 3. City Standardization ---\n",
    "city_mapping = {\n",
    "    'jkt': 'Jakarta', 'jakarta': 'Jakarta', 'jakrta': 'Jakarta',\n",
    "    'jakrta sel.': 'Jakarta', 'jakarta selatan': 'Jakarta',\n",
    "    'bandung': 'Bandung', 'surabaya': 'Surabaya'\n",
    "}\n",
    "df_clean['city'] = df_clean['city'].astype(str).str.lower().str.strip().map(\n",
    "    lambda x: city_mapping.get(x, x.title() if x != \"nan\" else \"Unknown\")\n",
    ")\n",
    "\n",
    "# --- 4. Numeric Cleaning (Total Orders & Avg Order Value) ---\n",
    "def clean_numeric(x):\n",
    "    if pd.isna(x) or x == \"\": \n",
    "        return 0\n",
    "    try:\n",
    "        return float(x)\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "df_clean[\"total_orders\"] = df_clean[\"total_orders\"].apply(clean_numeric).abs().astype(int)\n",
    "\n",
    "df_clean[\"avg_order_value\"] = (\n",
    "    df_clean[\"avg_order_value\"]\n",
    "    .astype(str)\n",
    "    .str.replace(\",\", \"\")\n",
    "    .apply(clean_numeric)\n",
    ")\n",
    "df_clean.loc[df_clean[\"avg_order_value\"] > 1e8, \"avg_order_value\"] = 0\n",
    "\n",
    "# --- 5. CANCEL RATE (Comprehensive Cleaning) ---\n",
    "def clean_cancel_rate(val):\n",
    "    if pd.isna(val): \n",
    "        return 0.0\n",
    "    \n",
    "    val_str = str(val).strip().replace(\" \", \"\").replace(\"%\", \"\")\n",
    "    \n",
    "    has_comma = ',' in val_str\n",
    "    has_dot = '.' in val_str\n",
    "    \n",
    "    if has_comma and has_dot:\n",
    "        comma_pos = val_str.rfind(',')\n",
    "        dot_pos = val_str.rfind('.')\n",
    "        \n",
    "        if comma_pos > dot_pos:\n",
    "            val_str = val_str.replace('.', '').replace(',', '.')\n",
    "        else:\n",
    "            val_str = val_str.replace(',', '')\n",
    "    \n",
    "    elif has_comma and not has_dot:\n",
    "        val_str = val_str.replace(',', '.')\n",
    "    \n",
    "    try:\n",
    "        num = float(val_str)\n",
    "    except:\n",
    "        return 0.0\n",
    "    \n",
    "    num = abs(num)\n",
    "    \n",
    "    if num > 10000:\n",
    "        str_num = str(int(num))\n",
    "        length = len(str_num)\n",
    "        num = num / (10 ** (length - 2))\n",
    "    \n",
    "    elif num > 100:\n",
    "        while num > 100:\n",
    "            num /= 10\n",
    "    \n",
    "    elif 0 < num <= 1:\n",
    "        num = num * 100\n",
    "    \n",
    "    if num > 100:\n",
    "        num = 100.0\n",
    "    \n",
    "    return round(num, 2)\n",
    "\n",
    "df_clean[\"cancel_rate\"] = df_clean[\"cancel_rate\"].apply(clean_cancel_rate)\n",
    "\n",
    "# --- 6. Categorical & Binary Cleaning ---\n",
    "return_map = {'dua': 2, 'tiga': 3, 'tiga puluh': 30, 'satu': 1}\n",
    "df_clean['return_count'] = (\n",
    "    df_clean['return_count']\n",
    "    .astype(str)\n",
    "    .str.lower()\n",
    "    .map(lambda x: return_map.get(x, x))\n",
    "    .apply(clean_numeric)\n",
    "    .astype(int)\n",
    ")\n",
    "\n",
    "def clean_cod(x):\n",
    "    s = str(x).lower().strip()\n",
    "    if s in ['1', 'true', 'ya', 'yes']: \n",
    "        return 1\n",
    "    if s in ['0', 'false', 'tidak', 'no']: \n",
    "        return 0\n",
    "    return np.nan\n",
    "\n",
    "df_clean['cod_failed'] = df_clean['cod_failed'].apply(clean_cod)\n",
    "\n",
    "# COD Failed Imputation (after cancel_rate cleaning)\n",
    "mask_nan = df_clean['cod_failed'].isna()\n",
    "df_clean.loc[mask_nan & (df_clean['cancel_rate'] > 25), 'cod_failed'] = 1\n",
    "df_clean.loc[mask_nan, 'cod_failed'] = 0\n",
    "df_clean['cod_failed'] = df_clean['cod_failed'].astype(int)\n",
    "\n",
    "# --- 7. Device ---\n",
    "device_mapping = {\n",
    "    'andriod': 'Android', 'android': 'Android',\n",
    "    'i-phone': 'iOS', 'iphone': 'iOS', 'ios': 'iOS',\n",
    "    'desktop': 'Desktop'\n",
    "}\n",
    "\n",
    "df_clean['device'] = (\n",
    "    df_clean['device']\n",
    "    .astype(str)\n",
    "    .str.lower()\n",
    "    .str.strip()\n",
    "    .map(lambda x: device_mapping.get(x, x) if x != \"nan\" else np.nan)\n",
    ")\n",
    "\n",
    "df_clean['complaints'] = df_clean['complaints'].apply(\n",
    "    lambda x: 1 if str(x).strip() not in ['0', 'nan', ''] else 0\n",
    ")\n",
    "\n",
    "# --- 8. RISK FLAG (Impute from cod_failed) ---\n",
    "df_clean['risk_flag'] = df_clean['risk_flag'].fillna(df_clean['cod_failed']).astype(int)\n",
    "\n",
    "print(f\"Cancel rate range: {df_clean['cancel_rate'].min():.2f} - {df_clean['cancel_rate'].max():.2f}\")\n",
    "print(df_clean['cancel_rate'].describe())\n",
    "print(f\"\\nRisk flag distribution after imputation:\")\n",
    "print(df_clean['risk_flag'].value_counts())\n",
    "print(f\"\\nData Cleaning Selesai.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Preprocessing\n",
    "Encoding, Scaling, Feature Engineering, dan Train-Test Split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Shape: (8171, 11)\n",
      "Test Shape: (2043, 11)\n"
     ]
    }
   ],
   "source": [
    "# Feature Engineering: Durasi Hari\n",
    "ref_date = df_clean['last_purchase'].max()\n",
    "df_clean['days_registered'] = (ref_date - df_clean['registration_date']).dt.days.fillna(0)\n",
    "df_clean['recency'] = (ref_date - df_clean['last_purchase']).dt.days.fillna(0)\n",
    "\n",
    "# Drop unused columns\n",
    "X = df_clean.drop(['customer_id', 'risk_flag', 'registration_date', 'last_purchase'], axis=1)\n",
    "y = df_clean['risk_flag']\n",
    "\n",
    "# Define Features\n",
    "cat_cols = ['city', 'device']\n",
    "num_cols = [c for c in X.columns if c not in cat_cols]\n",
    "\n",
    "# Preprocessing Pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), num_cols),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), cat_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Train-Test Split (80:20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(\"Train Shape:\", X_train.shape)\n",
    "print(\"Test Shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Benchmarking dengan Cross-Validation\n",
    "Kita akan menggunakan 5-Fold Cross Validation untuk menguji beberapa model.\n",
    "Metric yang dilihat: **Recall** (untuk meminimalkan False Negative)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking Models (Metric: Recall)...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Recall Mean</th>\n",
       "      <th>Recall Std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KNN</td>\n",
       "      <td>0.697579</td>\n",
       "      <td>0.016875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.675380</td>\n",
       "      <td>0.023769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.673280</td>\n",
       "      <td>0.025885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.668180</td>\n",
       "      <td>0.030516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.603969</td>\n",
       "      <td>0.021042</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Model  Recall Mean  Recall Std\n",
       "1                  KNN     0.697579    0.016875\n",
       "0  Logistic Regression     0.675380    0.023769\n",
       "3        Random Forest     0.673280    0.025885\n",
       "4              XGBoost     0.668180    0.030516\n",
       "2        Decision Tree     0.603969    0.021042"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42),\n",
    "    'KNN': KNeighborsClassifier(),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42),\n",
    "    'XGBoost': XGBClassifier(random_state=42, eval_metric='logloss')\n",
    "}\n",
    "\n",
    "results_cv = []\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "print(\"Benchmarking Models (Metric: Recall)...\")\n",
    "for name, model in models.items():\n",
    "    # Pipeline with SMOTE to handle imbalance during CV\n",
    "    pipeline = ImbPipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('smote', SMOTE(random_state=42)),\n",
    "        ('classifier', model)\n",
    "    ])\n",
    "    \n",
    "    scores = cross_val_score(pipeline, X_train, y_train, cv=skf, scoring='recall')\n",
    "    results_cv.append({\n",
    "        'Model': name,\n",
    "        'Recall Mean': scores.mean(),\n",
    "        'Recall Std': scores.std()\n",
    "    })\n",
    "\n",
    "df_cv = pd.DataFrame(results_cv).sort_values('Recall Mean', ascending=False)\n",
    "display(df_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Pilih Top 3 Model\n",
    "Berdasarkan skor Recall rata-rata dari Cross-Validation di atas, kita akan memilih 3 model terbaik untuk tahap selanjutnya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 3 Models: ['KNN', 'Logistic Regression', 'Random Forest']\n"
     ]
    }
   ],
   "source": [
    "top_3_models = df_cv.head(3)['Model'].tolist()\n",
    "print(\"Top 3 Models:\", top_3_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Penanganan Imbalance Learning\n",
    "Kita menggunakan **SMOTE (Synthetic Minority Over-sampling Technique)**.\n",
    "\n",
    "**Alasan:**\n",
    "Dataset risiko biasanya tidak seimbang (jumlah customer berisiko jauh lebih sedikit daripada customer aman). Jika tidak ditangani, model akan cenderung bias ke kelas mayoritas (memprediksi semua aman), yang akan menghasilkan banyak False Negative. SMOTE membantu menyeimbangkan kelas dengan membuat data sintetis untuk kelas minoritas saat training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Hyperparameter Tuning\n",
    "Kita akan melakukan tuning pada Top 3 model menggunakan `RandomizedSearchCV` atau `GridSearchCV` untuk mencari parameter optimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning Hyperparameters...\n",
      "\n",
      "KNN tuned:\n",
      "  Best Recall CV: 0.7015\n",
      "  Best Parameters:\n",
      "    - classifier__n_neighbors: 7\n",
      "\n",
      "Logistic Regression tuned:\n",
      "  Best Recall CV: 0.6754\n",
      "  Best Parameters:\n",
      "    - classifier__C: 0.1\n",
      "\n",
      "Random Forest tuned:\n",
      "  Best Recall CV: 0.6763\n",
      "  Best Parameters:\n",
      "    - classifier__n_estimators: 100\n",
      "    - classifier__min_samples_split: 2\n",
      "    - classifier__max_depth: None\n",
      "\n",
      "============================================================\n",
      "SUMMARY - Best Recall CV Scores:\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Best Recall CV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KNN</td>\n",
       "      <td>0.701470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.676268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.675368</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Model  Best Recall CV\n",
       "0                  KNN        0.701470\n",
       "2        Random Forest        0.676268\n",
       "1  Logistic Regression        0.675368"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Parameter Grids untuk Top 3\n",
    "param_grids = {\n",
    "    'Random Forest': {\n",
    "        'classifier__n_estimators': [100, 200],\n",
    "        'classifier__max_depth': [10, 20, None],\n",
    "        'classifier__min_samples_split': [2, 5]\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'classifier__n_estimators': [100, 200],\n",
    "        'classifier__learning_rate': [0.01, 0.1],\n",
    "        'classifier__max_depth': [3, 6]\n",
    "    },\n",
    "    'Decision Tree': {\n",
    "        'classifier__max_depth': [5, 10, None],\n",
    "        'classifier__min_samples_leaf': [1, 2, 4]\n",
    "    },\n",
    "    'Logistic Regression': {\n",
    "        'classifier__C': [0.1, 1, 10]\n",
    "    },\n",
    "    'KNN': {\n",
    "        'classifier__n_neighbors': [3, 5, 7]\n",
    "    }\n",
    "}\n",
    "\n",
    "best_estimators = {}\n",
    "tuned_results = []\n",
    "\n",
    "print(\"Tuning Hyperparameters...\")\n",
    "for name in top_3_models:\n",
    "    model = models[name]\n",
    "    pipeline = ImbPipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('smote', SMOTE(random_state=42)),\n",
    "        ('classifier', model)\n",
    "    ])\n",
    "    \n",
    "    search = RandomizedSearchCV(\n",
    "        pipeline, \n",
    "        param_distributions=param_grids.get(name, {}), \n",
    "        n_iter=5, \n",
    "        cv=3, \n",
    "        scoring='recall', \n",
    "        random_state=42, \n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    search.fit(X_train, y_train)\n",
    "    best_estimators[name] = search.best_estimator_\n",
    "    tuned_results.append({\n",
    "        'Model': name,\n",
    "        'Best Recall CV': search.best_score_\n",
    "    })\n",
    "    \n",
    "    # Print parameters dengan jelas\n",
    "    print(f\"\\n{name} tuned:\")\n",
    "    print(f\"  Best Recall CV: {search.best_score_:.4f}\")\n",
    "    print(f\"  Best Parameters:\")\n",
    "    for param, value in search.best_params_.items():\n",
    "        print(f\"    - {param}: {value}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUMMARY - Best Recall CV Scores:\")\n",
    "print(\"=\"*60)\n",
    "df_tuned = pd.DataFrame(tuned_results).sort_values('Best Recall CV', ascending=False)\n",
    "display(df_tuned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Testing ke Data Test & 11. Komparasi Before vs After\n",
    "Kita akan mengevaluasi model sebelum dan sesudah tuning pada data test (yang tidak dilihat saat training/tuning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Recall Default</th>\n",
       "      <th>Recall Tuned</th>\n",
       "      <th>Improvement</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KNN</td>\n",
       "      <td>0.711885</td>\n",
       "      <td>0.702281</td>\n",
       "      <td>-0.009604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.661465</td>\n",
       "      <td>0.661465</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.656663</td>\n",
       "      <td>0.656663</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Model  Recall Default  Recall Tuned  Improvement\n",
       "0                  KNN        0.711885      0.702281    -0.009604\n",
       "1  Logistic Regression        0.661465      0.661465     0.000000\n",
       "2        Random Forest        0.656663      0.656663     0.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "final_comparison = []\n",
    "\n",
    "for name in top_3_models:\n",
    "    # Before Tuning (Default)\n",
    "    default_pipe = ImbPipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('smote', SMOTE(random_state=42)),\n",
    "        ('classifier', models[name])\n",
    "    ])\n",
    "    default_pipe.fit(X_train, y_train)\n",
    "    y_pred_def = default_pipe.predict(X_test)\n",
    "    recall_def = recall_score(y_test, y_pred_def)\n",
    "    \n",
    "    # After Tuning\n",
    "    tuned_model = best_estimators[name]\n",
    "    y_pred_tuned = tuned_model.predict(X_test)\n",
    "    recall_tuned = recall_score(y_test, y_pred_tuned)\n",
    "    \n",
    "    final_comparison.append({\n",
    "        'Model': name,\n",
    "        'Recall Default': recall_def,\n",
    "        'Recall Tuned': recall_tuned,\n",
    "        'Improvement': recall_tuned - recall_def\n",
    "    })\n",
    "\n",
    "df_comparison = pd.DataFrame(final_comparison)\n",
    "display(df_comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Final Best Model Decision\n",
    "\n",
    "Berdasarkan hasil komparasi di atas, kita memilih model terbaik.\n",
    "\n",
    "**Kriteria Pemilihan:**\n",
    "1. **Recall Tertinggi**: Prioritas utama untuk menangkap sebanyak mungkin customer berisiko.\n",
    "2. **Stabilitas**: Perbedaan antara skor Train/CV dan Test tidak terlalu jauh (tidak overfitting).\n",
    "3. **Kompleksitas**: Jika performa mirip, pilih model yang lebih sederhana (misal: Logistic Regression/Decision Tree vs Random Forest).\n",
    "\n",
    "*(Isi bagian ini secara manual setelah melihat output tabel di atas saat running)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Limitasi Model\n",
    "1. **Data Noise**: Masih ada kemungkinan kesalahan input manual pada data historis yang mempengaruhi kualitas prediksi.\n",
    "2. **Imbalance Ekstrem**: Meskipun sudah di-SMOTE, jika rasio kelas sangat timpang, model mungkin masih menghasilkan False Positive yang cukup tinggi demi mengejar Recall.\n",
    "3. **Perubahan Perilaku**: Model dilatih pada data masa lalu. Jika perilaku penipuan berubah (pola baru), model perlu di-retrain secara berkala.\n",
    "4. **Fitur Terbatas**: Kita hanya menggunakan fitur transaksi dasar. Fitur perilaku (klik, durasi sesi) mungkin bisa meningkatkan akurasi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Impact Model ke Bisnis\n",
    "\n",
    "**Simulasi Dampak:**\n",
    "Misalkan tanpa model, kita meloloskan semua COD. \n",
    "- Total Kerugian = (Jumlah Bad Customer) x (Rata-rata Ongkir + Handling Cost).\n",
    "\n",
    "Dengan model (asumsi Recall 80%):\n",
    "- Kita berhasil mencegah 80% dari potensi kerugian tersebut.\n",
    "- **Efisiensi Operasional**: Tim CS tidak perlu memverifikasi semua order, cukup fokus pada yang diprediksi \"Berisiko\" oleh model.\n",
    "- **Keputusan Bisnis**: \n",
    "  - Jika Prediksi = Risiko Tinggi -> Matikan opsi COD, wajibkan Transfer Bank.\n",
    "  - Jika Prediksi = Aman -> Lanjutkan proses otomatis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
